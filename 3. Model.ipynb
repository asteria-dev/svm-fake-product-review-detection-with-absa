{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBqYyVHsWoQZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "# **MODEL IMPLEMENTATION AND EVALUATION**\n",
    "This is the stage where the three models are built, optimized and evaluated.\n",
    "\n",
    "**Models used:**  Support Vector Machine, Naive Bayes, KNN, Decision Trees\n",
    "\n",
    "**Evaluation methods used:** accuracy, precision, recall, f1_score and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiiNwn5IVNQQ"
   },
   "outputs": [],
   "source": [
    "#LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5paWIm9UXQ_Q"
   },
   "outputs": [],
   "source": [
    "#lOADING DATASETS\n",
    "df = pd.read_json('data/final_pre_process.json',encoding='latin1') #due to special charas should be encoded as latin 1\n",
    "\n",
    "#REMOVE MAX\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-IMKWarXUtw"
   },
   "source": [
    "## **DOUBLE-CHECKING...**\n",
    "Double checking if there are any NULL values within the dataset. This would cause issues later on if there are as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "AalLg05SXXsd",
    "outputId": "d6fa6b58-5bde-488a-912f-c5df7068bc90"
   },
   "outputs": [],
   "source": [
    "#CHECKING WHICH ROW IS NULL FROM PRE-PROCESSING\n",
    "checkNULL = df.isnull()\n",
    "checkNULL = checkNULL.any(axis=1)\n",
    "df[checkNULL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtDOogqAXfTe"
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATED VP VALUES\n",
    "df[\"verified\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr7fW5OxX791"
   },
   "source": [
    "The change barely had any affect on the T/F values, and thus we are ready to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1RRuZukXLwX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **SVM (ABSA)**\n",
    "\n",
    "INPUTS = REVIEWTEXT , MEAN ABSA, OVERALL(RATING)\n",
    "\n",
    "COVERS BOTH COUNT VECTORIZER AND TFIDF VECTORIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukzm7riST-Rf"
   },
   "source": [
    "## **Count Vectorizer**\n",
    "word vectorization maps words or phrases from a lexicon to a matching vector of real numbers, which may then be used to determine word predictions and semantics, and this is done due to the fact that models only understand numerical data.\n",
    "\n",
    "We are going to be utlizing two of the vectorization methods, the first one being count vectorizer. We just count the number of times a word appears in the document in CountVectorizer, which results in a bias in favor of the most common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Overall Sentiment', axis=1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvngKGlAUKqF"
   },
   "source": [
    "CHANGE TO STRING AS TRAINING DOES NOT ACCEPT NUMBERED NAMED COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUCy3HFKUJTm"
   },
   "outputs": [],
   "source": [
    "df['overall'] = df['overall'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8vCCYGvYCDf"
   },
   "source": [
    "### **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c1LpRtZnXqK"
   },
   "outputs": [],
   "source": [
    "#creating dummy variable for category class\n",
    "dummy_creat = pd.concat([df, pd.get_dummies(df['overall'])], axis=1)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "comment_feature = count_vect.fit_transform(dummy_creat['reviewText'])\n",
    "\n",
    "text_feature_df = pd.DataFrame(comment_feature.todense(), columns = count_vect.get_feature_names_out())\n",
    "\n",
    "cv_final_feature_df = pd.concat([text_feature_df, dummy_creat[['Mean ABSA Sentiment', '1', '2', '3', '4', '5', 'verified']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_final_feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKiZRamvUgkK"
   },
   "source": [
    "### **DATASET SPLIT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saA5ku5ASf6U"
   },
   "source": [
    "SPLIT DATASET 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6wizQ6_Uf6q"
   },
   "outputs": [],
   "source": [
    "train_x_cv = cv_final_feature_df[cv_final_feature_df.columns[:-1]]\n",
    "train_y_cv = cv_final_feature_df['verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6uzKS5BpCKa"
   },
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(\n",
    "    train_x_cv, train_y_cv,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkbAq6eaYfdQ"
   },
   "source": [
    "The data is decided to be split into 80 - 20, which has been determined by trial and error. This splitting produces the highest accuracy for the models, and thus we are going to with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hRq5TlcZirj"
   },
   "source": [
    "### **SVM (ABSA + COUNT VECTORIZER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSA TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3UxfVMcZla-"
   },
   "outputs": [],
   "source": [
    "#IMPLEMENTING AND RUNNNING SVM MODEL - COUNT\n",
    "\n",
    "svm1 = LinearSVC(random_state=0, tol=1e-5)\n",
    "svm1.fit(X_train_cv, y_train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION & EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajww0zq-u148"
   },
   "outputs": [],
   "source": [
    "#PREDICTION\n",
    "prediction = svm1.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "by9E70auZmrU"
   },
   "outputs": [],
   "source": [
    "#EVALUATION\n",
    "svm_a1 = accuracy_score(y_test_cv, prediction)*100\n",
    "svm_p1 = precision_score(y_test_cv, prediction)* 100\n",
    "svm_r1 = recall_score(y_test_cv, prediction)*100\n",
    "svm_f11 = f1_score(y_test_cv, prediction)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test_cv, prediction, labels=svm1.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=svm1.classes_)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", svm_a1)\n",
    "print(\"Precision: \", svm_p1)\n",
    "print(\"Recall: \", svm_r1)\n",
    "print(\"F1 Score: \", svm_f11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eekOpuTjZ045"
   },
   "source": [
    "## **TFIDF VECTORIZER**\n",
    "We examine the total document weightage of a word in TfidfVectorizer. It assists us in coping with the most common terms. We may use it to penalize them. The word counts are weighted by a measure of how frequently they appear in the documents in TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ih5Z6_B6TROj"
   },
   "outputs": [],
   "source": [
    "#creating dummy variable for category class\n",
    "dummy_creat = pd.concat([df, pd.get_dummies(df['overall'])], axis=1)\n",
    "\n",
    "tfid_vect = TfidfVectorizer(stop_words='english')\n",
    "comment_feature = tfid_vect.fit_transform(dummy_creat['reviewText'])\n",
    "\n",
    "text_feature_df = pd.DataFrame(comment_feature.todense(), columns = tfid_vect.get_feature_names_out())\n",
    "\n",
    "tfidf_final_feature_df = pd.concat([text_feature_df, dummy_creat[['Mean ABSA Sentiment', '1', '2', '3', '4', '5', 'verified']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDu3HrhEWcOt"
   },
   "source": [
    "### **DATASET SPLIT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrawK5SuWWyr"
   },
   "source": [
    "SPLIT DATASET 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5RuHrVXWeZ2"
   },
   "outputs": [],
   "source": [
    "train_x_tfidf = tfidf_final_feature_df[tfidf_final_feature_df.columns[:-1]]\n",
    "train_y_tfidf = tfidf_final_feature_df['verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UodHIsbWh_0"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
    "    train_x_tfidf, train_y_tfidf,test_size=0.22, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xtest_tfidf = X_test_tfidf.copy().reset_index(drop=True)\n",
    "new_ytest_tfidf = y_test_tfidf.copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmQ0mfphaAvJ"
   },
   "source": [
    "### **SVM (ABSA + TFIDF VECTORIZER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WV-2ZqkAaJhz"
   },
   "outputs": [],
   "source": [
    "#IMPLEMENTING AND RUNNNING SVM MODEL - COUNT\n",
    "svm1 = LinearSVC(random_state=0, tol=1e-5)\n",
    "svm1.fit(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxNTcHhsW4rV"
   },
   "source": [
    "**PREDICTION & EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHUChOeoW2ZO"
   },
   "outputs": [],
   "source": [
    "#PREDICTION\n",
    "prediction = svm1.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQqP-IXFaKj5"
   },
   "outputs": [],
   "source": [
    "#EVALUATION\n",
    "svm_a2 = accuracy_score(y_test_tfidf, prediction)*100\n",
    "svm_p2 = precision_score(y_test_tfidf, prediction)* 100\n",
    "svm_r2 = recall_score(y_test_tfidf, prediction)*100\n",
    "svm_f12 = f1_score(y_test_tfidf, prediction)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test_tfidf, prediction, labels=svm1.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=svm1.classes_)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", svm_a2)\n",
    "print(\"Precision: \", svm_p2)\n",
    "print(\"Recall: \", svm_r2)\n",
    "print(\"F1 Score: \", svm_f12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4_8GqyGXqTw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **SVM (NON-ABSA)**\n",
    "\n",
    "INPUTS = REVIEWTEXT\n",
    "\n",
    "COVERS BOTH COUNT VECTORIZER AND TFIDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PR__d_KlYOFN"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data/final_pre_process.json',encoding='latin1') #due to special charas should be encoded as latin 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Errv9dhOYX4D"
   },
   "source": [
    "**RECHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmmrzZIXYPyz"
   },
   "outputs": [],
   "source": [
    "#CHECKING WHICH ROW IS NULL FROM PRE-PROCESSING\n",
    "%timeit\n",
    "checkNULL = df.isnull()\n",
    "checkNULL = checkNULL.any(axis=1)\n",
    "df[checkNULL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATED VP VALUES\n",
    "df[\"verified\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHEQZZiRYUj1"
   },
   "outputs": [],
   "source": [
    "#DROP DUPLICATES\n",
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVprxs36YaS3"
   },
   "source": [
    "**MODELING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGuHNUSAXwmF"
   },
   "outputs": [],
   "source": [
    "#ASSIGN THE VARIABLES\n",
    "X = df['reviewText'] #input var\n",
    "y = df['verified'] #target var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT DATA\n",
    "%timeit\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['reviewText'], df['verified'],test_size=0.2, random_state=42) #40% gives best results, 42 is no of life...\n",
    "\n",
    "entiredf = format(df.shape[0])\n",
    "traindf = format(X_train.shape[0])\n",
    "testdf = format(X_test.shape[0])\n",
    "\n",
    "print('Number of rows:')\n",
    "print('Entire dataset:', entiredf)\n",
    "print('Train dataset:', traindf)\n",
    "print('Test dataset:',testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrNXyBvvYlqt",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **COUNT VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCxrGKn0YqFf"
   },
   "outputs": [],
   "source": [
    "count_vectorizer  = CountVectorizer(stop_words='english')\n",
    "count_vectorizer.fit(X_train)\n",
    "print('\\nVocabulary: \\n', count_vectorizer.vocabulary_)\n",
    "\n",
    "train_c = count_vectorizer.fit_transform(X_train)\n",
    "test_c = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iA0sqpMLY1Sz",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **SVM (COUNT VECTORIZER NON ABSA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION & EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za6WNKeDY60L"
   },
   "outputs": [],
   "source": [
    "#IMPLEMENTING AND RUNNNING SVM MODEL - COUNT\n",
    "svm1 = LinearSVC(random_state=0, tol=1e-5)\n",
    "svm1.fit(train_c, y_train)\n",
    "prediction = svm1.predict(test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SilgJ31RZHuY"
   },
   "outputs": [],
   "source": [
    "#EVALUATION\n",
    "svm_na_a1 = accuracy_score(y_test, prediction)*100\n",
    "svm_na_p1 = precision_score(y_test, prediction)* 100\n",
    "svm_na_r1 = recall_score(y_test, prediction)*100\n",
    "svm_na_f11 = f1_score(y_test, prediction)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test, prediction, labels=svm1.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=svm1.classes_)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", svm_na_a1)\n",
    "print(\"Precision: \", svm_na_p1)\n",
    "print(\"Recall: \", svm_na_r1)\n",
    "print(\"F1 Score: \", svm_na_f11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuOoPmzpZQqi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **TFIDF VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hC9LrWKZSSS"
   },
   "outputs": [],
   "source": [
    "TFIDF_vectorizer  = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "TFIDF_vectorizer.fit(X_train)\n",
    "print('\\nVocabulary: \\n', TFIDF_vectorizer.vocabulary_)\n",
    "\n",
    "train_tf = TFIDF_vectorizer.fit_transform(X_train)\n",
    "test_tf = TFIDF_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WYzXuVOZa0i",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **SVM (TFIDF VECTORIZER NON ABSA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION & EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6egHzWIDZc-f"
   },
   "outputs": [],
   "source": [
    "#IMPLEMENTING AND RUNNING SVM MODEL - TFIDF\n",
    "svm2 = LinearSVC(random_state=0, tol=1e-5)\n",
    "svm2.fit(train_tf, y_train)\n",
    "prediction = svm2.predict(test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vROsfJBDZiSu"
   },
   "outputs": [],
   "source": [
    "#EVALUATION\n",
    "svm_na_a2 = accuracy_score(y_test, prediction)*100\n",
    "svm_na_p2 = precision_score(y_test, prediction)* 100\n",
    "svm_na_r2 = recall_score(y_test, prediction)*100\n",
    "svm_na_f12 = f1_score(y_test, prediction)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test, prediction, labels=svm2.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=svm2.classes_)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", svm_na_a2)\n",
    "print(\"Precision: \", svm_na_p2)\n",
    "print(\"Recall: \", svm_na_r2)\n",
    "print(\"F1 Score: \", svm_na_f12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **NAIVE BAYES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **COUNT VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_c, y_train)\n",
    "prediction = nb.predict(test_c)\n",
    "\n",
    "#EVALUATION\n",
    "nb_a1 = accuracy_score(y_test, prediction)*100\n",
    "nb_p1 = precision_score(y_test, prediction)* 100\n",
    "nb_r1 = recall_score(y_test, prediction)*100\n",
    "nb_f11 = f1_score(y_test, prediction)*100\n",
    "\n",
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test, prediction, labels=nb.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=nb.classes_)\n",
    "display.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", nb_a1)\n",
    "print(\"Precision: \", nb_p1)\n",
    "print(\"Recall: \", nb_r1)\n",
    "print(\"F1 Score: \", nb_f11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TFIDF VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_tf, y_train)\n",
    "prediction = nb.predict(test_tf)\n",
    "\n",
    "#EVALUATION\n",
    "nb_a2 = accuracy_score(y_test, prediction)*100\n",
    "nb_p2 = precision_score(y_test, prediction)* 100\n",
    "nb_r2 = recall_score(y_test, prediction)*100\n",
    "nb_f12 = f1_score(y_test, prediction)*100\n",
    "\n",
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test, prediction, labels=nb.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=nb.classes_)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", nb_a1)\n",
    "print(\"Precision: \", nb_p1)\n",
    "print(\"Recall: \", nb_r1)\n",
    "print(\"F1 Score: \", nb_f11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **KNN MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WYzXuVOZa0i"
   },
   "source": [
    "## **COUNT VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_c, y_train)\n",
    "prediction = knn.predict(test_c)\n",
    "\n",
    "#EVALUATION\n",
    "knn_a1 = accuracy_score(y_test, prediction)*100\n",
    "knn_p1 = precision_score(y_test, prediction)* 100\n",
    "knn_r1 = recall_score(y_test, prediction)*100\n",
    "knn_f11 = f1_score(y_test, prediction)*100\n",
    "\n",
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test, prediction, labels=knn.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=knn.classes_)\n",
    "display.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", knn_a1)\n",
    "print(\"Precision: \", knn_p1)\n",
    "print(\"Recall: \", knn_r1)\n",
    "print(\"F1 Score: \", knn_f11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TFIDF VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_tf, y_train)\n",
    "prediction = knn.predict(test_tf)\n",
    "\n",
    "#EVALUATION\n",
    "knn_a2 = accuracy_score(y_test, prediction)*100\n",
    "knn_p2 = precision_score(y_test, prediction)* 100\n",
    "knn_r2 = recall_score(y_test, prediction)*100\n",
    "knn_f12 = f1_score(y_test, prediction)*100\n",
    "\n",
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test, prediction, labels=knn.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=knn.classes_)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", knn_a2)\n",
    "print(\"Precision: \", knn_p2)\n",
    "print(\"Recall: \", knn_r2)\n",
    "print(\"F1 Score: \", knn_f12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **DECISSION TREE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **COUNT VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_c, y_train)\n",
    "prediction = dt.predict(test_c)\n",
    "\n",
    "#EVALUATION\n",
    "dt_a1 = accuracy_score(y_test, prediction)*100\n",
    "dt_p1 = precision_score(y_test, prediction)* 100\n",
    "dt_r1 = recall_score(y_test, prediction)*100\n",
    "dt_f11 = f1_score(y_test, prediction)*100\n",
    "\n",
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test, prediction, labels=dt.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=dt.classes_)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDecision Tree - Count Vectorizer')\n",
    "print('Accuracy:', dt_a1)\n",
    "print('Precision:', dt_p1)\n",
    "print('Recall:', dt_r1)\n",
    "print('F1 Score:', dt_f11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TFIDF VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt * DecisionTreeClassifier()\n",
    "dt.fit(train_tf, y_train)\n",
    "prediction = dt.predict(test_tf)\n",
    "\n",
    "#EVALUATION\n",
    "dt_a2 = accuracy_score(y_test, prediction)*100\n",
    "dt_p2 = precision_score(y_test, prediction)* 100   \n",
    "dt_r2 = recall_score(y_test, prediction)*100\n",
    "dt_f12 = f1_score(y_test, prediction)*100\n",
    "\n",
    "#CONFUSION MATRIX\n",
    "cm =  confusion_matrix(y_test, prediction, labels=dt.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=dt.classes_)\n",
    "display.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDecision Tree - TFIDF Vectorizer')\n",
    "print('Accuracy:', dt_a2)\n",
    "print('Precision:', dt_p2)\n",
    "print('Recall:', dt_r2)\n",
    "print('F1 Score:', dt_f12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **DISPLAY RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPLAYING THE RESULTS\n",
    "print('SVM ABSA- Count Vectorizer')\n",
    "print('Accuracy:', svm_a1)\n",
    "print('Precision:', svm_p1)\n",
    "print('Recall:', svm_r1)\n",
    "print('F1 Score:', svm_f11)\n",
    "\n",
    "print('\\nSVM ABSA - TFIDF Vectorizer')\n",
    "print('Accuracy:', svm_a2)\n",
    "print('Precision:', svm_p2)\n",
    "print('Recall:', svm_r2)\n",
    "print('F1 Score:', svm_f12)\n",
    "\n",
    "print('\\nSVM NON-ABSA- Count Vectorizer')\n",
    "print('Accuracy:', svm_na_a1)\n",
    "print('Precision:', svm_na_p1)\n",
    "print('Recall:', svm_na_r1)\n",
    "print('F1 Score:', svm_na_f11)\n",
    "\n",
    "print('\\nSVM NON-ABSA - TFIDF Vectorizer')\n",
    "print('Accuracy:', svm_na_a2)\n",
    "print('Precision:', svm_na_p2)\n",
    "print('Recall:', svm_na_r2)\n",
    "print('F1 Score:', svm_na_f12)\n",
    "\n",
    "print('\\nNaive Bayes - Count Vectorizer')\n",
    "print('Accuracy:', nb_a1)\n",
    "print('Precision:', nb_p1)\n",
    "print('Recall:', nb_r1)\n",
    "print('F1 Score:', nb_f11)\n",
    "\n",
    "print('\\nNaive Bayes - TFIDF Vectorizer')\n",
    "print('Accuracy:', nb_a2)\n",
    "print('Precision:', nb_p2)\n",
    "print('Recall:', nb_r2)\n",
    "print('F1 Score:', nb_f12)\n",
    "\n",
    "print('\\nKNN - Count Vectorizer')\n",
    "print('Accuracy:', knn_a1)\n",
    "print('Precision:', knn_p1)\n",
    "print('Recall:', knn_r1)\n",
    "print('F1 Score:', knn_f11)\n",
    "\n",
    "print('\\nKNN - TFIDF Vectorizer')\n",
    "print('Accuracy:', knn_a2)\n",
    "print('Precision:', knn_p2)\n",
    "print('Recall:', knn_r2)\n",
    "print('F1 Score:', knn_f12)\n",
    "\n",
    "print('\\nDecision Tree - Count Vectorizer')\n",
    "print('Accuracy:', dt_a1)\n",
    "print('Precision:', dt_p1)\n",
    "print('Recall:', dt_r1)\n",
    "print('F1 Score:', dt_f11)\n",
    "\n",
    "print('\\nDecision Tree - TFIDF Vectorizer')\n",
    "print('Accuracy:', dt_a2)\n",
    "print('Precision:', dt_p2)\n",
    "print('Recall:', dt_r2)\n",
    "print('F1 Score:', dt_f12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "z1RRuZukXLwX",
    "_hRq5TlcZirj",
    "eekOpuTjZ045",
    "JDu3HrhEWcOt",
    "pmQ0mfphaAvJ",
    "F4_8GqyGXqTw"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
